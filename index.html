<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>System Architecture for Synthetic/Natural Hybrid Coding </title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Montserrat:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/icofont/icofont.min.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/venobox/venobox.css" rel="stylesheet">
  <link href="assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Knight - v3.0.0
  * Template URL: https://bootstrapmade.com/knight-free-bootstrap-theme/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Hero Section ======= -->
  <section id="hero">
    <div class="hero-container">
      <a href="index.html" class="hero-logo" data-aos="zoom-in"><img src="assets/img/logo.png" alt=""></a>
      <h1 data-aos="zoom-in">Multimedia System</h1>
      <h2 data-aos="fade-up">System Architecture for Synthetic/Natural Hybrid Coding </h2>
      <a data-aos="fade-up" href="#You" class="btn-get-started scrollto">Start Learning</a>
	  <a data-aos="fade-up" href="index2.html" class="btn-get-started scrollto">References</a>
    </div>
  </section><!-- End Hero -->

  <!-- ======= Header ======= -->
  <header id="header" class="d-flex align-items-center">
    <div class="container">

      <!-- The main logo is shown in mobile version only. The centered nav-logo in nav menu is displayed in desktop view  -->
      <div class="logo d-block d-lg-none">
        <a href="index.html"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>
      </div>

      <nav class="nav-menu d-none d-lg-block">
        <ul class="nav-inner">
          <li><a href="https://bukoroapps.github.io/Multimedia/index.html">Home</a></li>

		  <li><a href="#You">SNHC</a></li>
          <li><a href="#services">SCENE-DESCRIPTION CAPABILITY </a></li>
		  <li><a href="#service">SYSTEM ARCHITECTURE FOR SNHC</a></li>
		  <li><a href="#serviced">TERMINAL ARCHITECTURE</a></li>
		  <li><a href="#serviceds">NETWORK ARCHITECTURE </a></li>

          <li class="nav-logo"><a href="index.html"><img src="assets/img/logo.png" alt="" class="img-fluid"></a></li>

        </ul>
      </nav><!-- .nav-menu -->

    </div>
  </header><!-- End Header -->

  <main id="main">

  
	
    <!-- ======= About Us Section ======= -->
    <section id="You" class="you">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>System Architecture for Synthetic/Natural Hybrid Coding </h2>
<p>RECENTLY, both digital video and computer graphics have evolved rapidly. Interaction capabilities, previously exploited in computer graphics, are now emphasized in digital video Compression and streaming technologies that were mainly developed in digital video are now focused on computer graphics. </p>
		  </div>
        <div class="row">
          <div class="col-lg-6" data-aos="fade-right">
          </div>
     
              <h5>MPEG-4 specifies synthetic/natural hybrid coding (SNHC)</h5>
              <ul>
<li><i class="bx bx-check-double"></i>MPEG-4 specifies synthetic/natural hybrid coding (SNHC) in their compression algorithms (facial animation and mesh compression) and their binary format for scene (BIFS) description [1], [2]. They collaborate with virtual reality modeling language (VRML) [3], which specifies a file format for describing three-dimensional (3-D) interactive worlds and objects exchanged on the Internet. </li>
<br>
<li><i class="bx bx-check-double"></i>Streaming technologies are mainly specified in the ITU-T H series, such as H.323 for packet networks including the Internet [4] and H.324 for general switched telephone networks and mobile channels [5]. </li>
<br>
<li><i class="bx bx-check-double"></i>The Internet Engineering Tasking Force (IETF) also specifies real-time transport protocol (RTP) [6], which is used for streaming applications on the Internet and composes a part of the ITU-T H.323 standard. However, there have been few discussions about system architecture for the synthetic/natural hybrid coding. </li>
<br>
<li><i class="bx bx-check-double"></i>MPEG-4 provides a framework but does not specify actual transport protocols. </li>
<br>
<li><i class="bx bx-check-double"></i>ITU-T and IETF present various protocols but do not give specific considerations on the hybrid coding. Such inconsistency isolates the hybrid coding from many practical the streaming technologies in a unified manner. </li>
<br>
<li><i class="bx bx-check-double"></i>Transmission of scene-description formats is investigated to incorporate synthetic objects (i.e., computer graphics) into the digital video world and to provide interaction capability. </li>
<br>
<li><i class="bx bx-check-double"></i>Terminal architecture with a media synchronization mechanism and network architecture considering existing transport protocols are also discussed, followed by considerations on technological gaps between digital video and computer graphics. Last, several experimental results are presented. </li>
	 
			 </ul>
            </div>
          </div>
        </div>

      </div>
    </section><!-- End About Us Section -->

    <!-- ======= Services Section ======= -->
    <section id="services" class="services">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>SCENE-DESCRIPTION CAPABILITY</h2>
        </div>
<p>
A. Paradigms video and computer graphics assuming network transmission.</p>
<p>
Digital video has evolved with compression and streaming technologies. Video signals are encoded and multiplexed with audio at the transmitter side. They are transferred into a network consecutively, then demultiplexed and decoded at the receiver side. </p>
<p>
Streaming mechanisms play an important role to guarantee synchronized presentation of video and audio [7]. On the other hand, computer graphics has evolved with technologies for synthetic object modeling and rendering. </p>
<p>
They are stored in the form of rendering programs or by a specified data-base format on a server. They are exchanged through a network, and a scene is generated incorporating synthetic objects at the receiver side. In general, animation is supported by a script or a byte code instead of streaming mechanisms. Interaction capability, thanks to the scene-description information, has been emphasized there [8], [9]. 
</p>
<p>
Integration of digital video and computer graphics has been carried out in several ways. An example is a virtual studio, in which composition is done at the transmitter side and the result is encoded as a single video source. An advantage of this approach is that the current paradigm of digital video does not have to be changed.</p>
<p>
A disadvantage, however, is elimination in the receiver of interaction capability, which is an important property of computer graphics. Another example is VRML, in which video sources are downloaded along with a scene-description format (i.e., a VRML file) beforehand. </p>
<p>
An advantage is that interaction capability is fully supported since the scene information is transferred to the receiver. Animation is carried out by script embedded in a VRML file. However, the receiver has to wait.
</p>
<p>
TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
</p>
<p>
Paradigms for digital video and computer graphics:  digital video, computer graphics, and integration of digital video and computer graphics for a long time for huge media sources to be downloaded completely due to the lack of streaming mechanisms. provides a new paradigm for this integration purpose description format is attached to video/audio streams.
</p> 
<p>They are encoded, multiplexed, and transferred to the receiver, and a scene is generated consecutively. Video and
audio streams are handled similarly. Interaction capability is preserved since the scene-description. data is transferred, similar. A technical challenge that both decoding and rendering have to be carried out continuously within an expected frame rate.
</p>
<p>
B. A Scene-Description Format for Streaming Media
</p>

<p>
Scene-description formats, such as HTML and VRML,
specify spatial/temporal relationships of objects inside a scene.
In the case of HTML, spatial relationship is determined implicitly according to appearance order. In the case of VRML, it is decided explicitly by the specified fields: center, translation, rotation, and scale. In both cases, temporal relationship is presented by attached byte codes or scripts, known as Java or ECMAScript, which contribute to interaction and animation. 
</p>

<p>
Standards for streaming applications, such as MPEG and ITU-T H series, specify compression algorithms and related control (system) parameters. The compression results are called elementary streams. The control parameters are used to manage elementary streams: multiplexing, buffer control, synchronization, and so on. They may be signaled on a different channel (i.e., control channel), compose an independent elementary stream, be attached to an access unit that is a piece of an elementary stream (e.g., time stamps for a video frame), or be transformed to other parameters according to transport protocols. 
</p>

<p>
Demonstrates a general method to incorporate streaming media into a scene-description format. It consists of scene description, media stream properties, and elementary streams. The scene-description part is composed of a scene graph and an optional program (script or byte code). A scene is described in a hierarchical manner by cascading nodes containing VRML-like parameters: center, translation, rotation, and scale. 
</p>
     
<p>
The node is a minimal unit of the scene graph, to which a unique identification number is assigned. The node is typically classified as a group node and a leaf node; the former has children, but the latter does not. The group node is used to group and reuse children inside a scene. The leaf node is used to specify detailed properties of synthetic objects, as well as video textures and audio sources, which are mapped onto objects. In VRML, the former corresponds to Transform and Group nodes, and the latter to Sphere, Box, Indexed Face-Set, Movie Texture, Audio Clip, and so on.
</p>
<p>
The media stream properties are containers of decoder control parameters. The parameters indicate compression algorithm, bit rates, buffer sizes, random access information, pointers to the elementary streams (e.g., URL’s), and so on. A unique identification number is assigned to the container and is used to associate the media to the scene description. Shows an example of scene-description syntax. The ID provides a unique identifier to a node in a scene graph, corresponding to that in the scene description. The type informs a node type, e.g., Transform or Sphere in the case of VRML. The node type determines subsequent
</p>	 
	 </div>
    </section><!-- End Services Section -->
	    <!-- ======= Services Section ======= -->
    <section id="service" class="service">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>SYSTEM ARCHITECTURE FOR SNHC</h2>
		  <p>Data structure according to the node definition.</p><br>
	 </div>
	 <p>
The count1 and count2 present the number of fields and the number of children nodes, respectively.
</p>
<br>
 <p>Field values are provided along with a field ID, which identifies a field complying with the node definition. 
 </p>
 <br>
 <p>
 Children nodes are specified by ID values, which are also defined in their own node descriptions.
 </p>
 <br>
<p> 
A nesting structure like VRML is not assumed here, but it is straightforwardly done by replacing the ID fields by node descriptions of children.
</p>
<br> 
<p>
Scene Update
</p>
<p>
Scene description itself can be considered as an elementary stream by defining its update syntax. Depicts this concept, in which an elementary stream of a scene graph and its control parameters are provided. Based on this mechanism, another animation mechanism analogous to the digital video
paradigm (I/P-pictures) is provided. 
</p>
<p>
Shows the corresponding syntax for scene update, to which time stamps may be attached when necessary. The ID indicates a node in a scene graph to be updated. The mode determines an action taken by the update command to the indicated node. The count specifies the number of subsequent components, of which data structure is determined by the mode field. Currently, four modes are considered: replace, append, insert, and remove. The former two are used to change field values in the selected node. 
</p>
<p>	
Replace sets new values and append adds values to previous ones. For each of them, a field ID is attached to identify a field in the node to be updated. Insert and remove are used to manage children nodes. The former inserts new children nodes. When the inserted child node has its own children, corresponding insert commands should be issued afterwards. Remove erases children nodes, which are identified by their ID values. It is assumed that all of the children are removed when their parent node is removed. 
</p>	
<p>	
Special cases have to be taken into account when insert and remove commands are applied to a node associated with media streams. In the case of insertion, media stream properties have to be informed along with the insert command. Insertion is carried out only after necessary operations (decoder initialization and channel opening) are completed. In the case of removal, it is applied only after corresponding channels are closed successfully. These problems are related to the control protocols with negotiation such as ITU-T H.245 [10]. D. Relationship to Existing Standards This section utilizes a generalized form to discuss scene description formats necessary to incorporate synthetic objects and to provide interaction capability. It follows a scene-graph concept adopted by VRML. Therefore, it can be applied to VRML and can be used as its streaming extension. In this case, scene description is done in a nesting manner, and node identification is executed through its naming mechanism.
</p>
<p>
The MPEG-4 system provides BIFS and object descriptors, which correspond to the scene-description format and the scene/media stream properties discussed in this section, respectively. Differences lie in the compactness of the proposed method. 
</p>
<p>
The current MPEG-4 specification provides more functionalities that have not been considered in this paper. For example, MPEG-4 provides two update mechanisms: BIFS Update, which happens at a given time instant, and BIFS-Anim for continuous change of node parameters. This separation contributes to saving bits in addition to their functional efficiency. 
</p>	
	
	</section><!-- End Services Section -->
	   
	   </section><!-- End Services Section -->
	    <!-- ======= Services Section ======= -->
    <section id="serviced" class="serviced">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>TERMINAL ARCHITECTURE</h2>
	 </div>
	 <p>
A. Browser (Decoder) Configuration 
</p>
 <p>
 A browser receives scene-description data along with video/audio streams. Control parameters are also attached for the decoder/compositor control. 
 </p>
 <p>
Note that the term “browser” is used instead of the classical term, decoder, since the decoder is a part of the browser, as shown. Demonstrates an example of browser configuration. Streams are separated by a demultiplexer into video, audio, scene-description, and control data. They are stored in buffers and decoded. The results are stored in memories, and their composition begins. A scene with synthetic objects is generated according to the scene-description format. 
 </p>
<p> 
Video and audio signals are then composed into a scene, and the result is presented to a viewer. The viewer may try to interact with the scene. The interaction, such as viewpoint movement, is reflected in subsequent scenes produced by the scene generator. A controller indicates the start of decoding and composition to decoders and a compositor according to control parameters, respectively. </p>
<p>
Stream properties of the scene and media have to be communicated to the browser when starting a session. They are used to initialize decoders and to open necessary channels. When a new object to which new media streams are associated is inserted during a session, a similar procedure is applied. 
</p>
<p>
Buffers store compressed data. Their recommended sizes should be specified through the media stream properties in order to avoid buffer overflow and underflow. Memories store decoding results. Their sizes may be implicitly determined by picture sizes or audio frame periods that are contained in the stream properties or elementary streams themselves. However, when variable delay networks are assumed and a pre-download mechanism of elementary streams is required, both buffer and memory sizes may be controlled by a server through explicit indications. 
</p>
<p>
B. Server (Encoder) Configuration 
</p>
<p>	
An example of a server (encoder) configuration is presented It takes the style of an authoring tool, in which user interaction is applied to a scene generator and the change is transferred to an encoder. The handling of video and audio streams is carried out in a similar way to the classical digital video paradigm. The streams are encoded and put into buffers. They are locally decoded and stored in memories. The data in the memories are used for encoding of subsequent access units and for both the encoder side and the decoder side cause variable delays. 
</p>	
<p>	
Then two time stamps, the decoding time stamp (DTS) and presentation time stamp (PTS), are specified to indicate synchronization points among different streaming media. They are attached to access units at the encoder side and retrieved by the decoder to know adequate decode and presentation timing. When necessary, time bases furthermore are transferred from the encoder to the decoder, the so-called clock references. This framework can be applied to the current case, in which a compositor is newly introduced. Buffers cause variable delays.</p>
<p>
Networks or storage devices cause constant or variable delays; circuit switching networks and storage devices cause constant delays, and packet networks such as the Internet cause variable delays. It is then assumed that, as far as any operations are carried out on the timeline with constant delays to the synchronization points, media synchronization is possible. Accordingly, three time stamps are introduced; </p>	
	
	</section><!-- End Services Section -->

	</section><!-- End Services Section -->
	    <!-- ======= Services Section ======= -->
    <section id="serviceds" class="serviceds">
      <div class="container">

        <div class="section-title" data-aos="fade-up">
          <h2>NETWORK ARCHITECTURE </h2>
	 </div>
	 <p>
A. Basic Scenario shows a mapping scenario of the proposed data structure to actual transport mechanisms. 
</p>
 <p>
There are already many transport protocols: for example, MPEG-2 transport stream [7], H.223 [11], H.225 [12], and RTP [6]. They introduce their own terms to classify data streams according to functionality and different quality-of-service requirements.  </p>
 <p>
In they are classified into either system layer and media layer or control channel, media channel, and data channel. The system layer handles the control aspects and is equivalent to the control channel. The media layer conveys actual streaming data and corresponds to the media channel.  </p>
<p> 
The data channel is optionally used to transfer private data that may be inserted into the system layer. It is generally assumed that both the system layer and the control channel are protected by some error-handling strategies: retransmission or error-correction codes.
</p>
<p>
This is because they convey important parameters that should not be lost. Scene and media stream properties are necessary to initialize decoders and a compositor. Therefore, they are assumed to be transmitted by the system layer or by the control channel. Scene elementary streams convey scene-description data, which also need reliable transmission. 
</p>
<p>
Therefore, they are assumed to be transferred by the system layer as private data or by the data channel with some error protection. Media channels may be also utilized when adequate error protection is applied. Media elementary streams convey compression streams for audio, video, or geometry animation such as facial animation or object transformation dynamics. Their quantity is sometimes quite large, and they should be conveyed by the media layer or by the media channel without heavy error protection.
</p>
	
	</section><!-- End Services Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">

    <div class="footer-top">

      <div class="container">

          </div>
        </div>



      </div>
    </div>

    <div class="container footer-bottom clearfix">
      <div class="copyright">

      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/knight-free-bootstrap-theme/ -->
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top"><i class="icofont-simple-up"></i></a>
        <!-- ======= Header ======= -->
  <header id="header" class="d-flex align-items-center">
    <div class="container">

      <!-- The main logo is shown in mobile version only. The centered nav-logo in nav menu is displayed in desktop view  -->
      <div class="logo d-block d-lg-none">
        <a href="index.html"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>
      </div>

      <nav class="nav-menu d-none d-lg-block">
        <ul class="nav-inner">
          <li><a href="index1.html">NEXT TOPIC</a></li>
          <li class="nav-logo"><a href="index1.html"><img src="assets/img/logo.png" alt="" class="img-fluid"></a></li>

        </ul>
      </nav><!-- .nav-menu -->

    </div>
  </header><!-- End Header -->

  <!-- Vendor JS Files -->
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/jquery-sticky/jquery.sticky.js"></script>
  <script src="assets/vendor/venobox/venobox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/owl.carousel/owl.carousel.min.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>